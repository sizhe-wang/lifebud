'use strict';

var chunkHQHGGAB7_cjs = require('./chunk-HQHGGAB7.cjs');
var chunkPPEP5ZHX_cjs = require('./chunk-PPEP5ZHX.cjs');
var chunkS65YV6YB_cjs = require('./chunk-S65YV6YB.cjs');
var chunk4U7ZLI36_cjs = require('./chunk-4U7ZLI36.cjs');
var chunkVW7YQWDW_cjs = require('./chunk-VW7YQWDW.cjs');
var chunkDGV2FWB4_cjs = require('./chunk-DGV2FWB4.cjs');
var schemaCompat = require('@mastra/schema-compat');
var zodToJson = require('@mastra/schema-compat/zod-to-json');
var crypto = require('crypto');

// src/stream/aisdk/v4/usage.ts
function convertV4Usage(usage) {
  if (!usage) {
    return {};
  }
  return {
    inputTokens: usage.promptTokens,
    outputTokens: usage.completionTokens
  };
}

// src/llm/model/model.ts
var MastraLLMV1 = class extends chunkDGV2FWB4_cjs.MastraBase {
  #model;
  #mastra;
  #options;
  constructor({ model, mastra, options }) {
    super({ name: "aisdk" });
    this.#model = model;
    this.#options = options;
    if (mastra) {
      this.#mastra = mastra;
      if (mastra.getLogger()) {
        this.__setLogger(this.#mastra.getLogger());
      }
    }
  }
  __registerPrimitives(p) {
    if (p.logger) {
      this.__setLogger(p.logger);
    }
  }
  __registerMastra(p) {
    this.#mastra = p;
  }
  getProvider() {
    return this.#model.provider;
  }
  getModelId() {
    return this.#model.modelId;
  }
  getModel() {
    return this.#model;
  }
  _applySchemaCompat(schema) {
    const model = this.#model;
    const schemaCompatLayers = [];
    if (model) {
      const modelInfo = {
        modelId: model.modelId,
        supportsStructuredOutputs: model.supportsStructuredOutputs ?? false,
        provider: model.provider
      };
      schemaCompatLayers.push(
        new schemaCompat.OpenAIReasoningSchemaCompatLayer(modelInfo),
        new schemaCompat.OpenAISchemaCompatLayer(modelInfo),
        new schemaCompat.GoogleSchemaCompatLayer(modelInfo),
        new schemaCompat.AnthropicSchemaCompatLayer(modelInfo),
        new schemaCompat.DeepSeekSchemaCompatLayer(modelInfo),
        new schemaCompat.MetaSchemaCompatLayer(modelInfo)
      );
    }
    return schemaCompat.applyCompatLayer({
      schema,
      compatLayers: schemaCompatLayers,
      mode: "aiSdkSchema"
    });
  }
  async __text({
    runId,
    messages,
    maxSteps = 5,
    tools = {},
    temperature,
    toolChoice = "auto",
    onStepFinish,
    experimental_output,
    threadId,
    resourceId,
    requestContext,
    tracingContext,
    ...rest
  }) {
    const model = this.#model;
    this.logger.debug(`[LLM] - Generating text`, {
      runId,
      messages,
      maxSteps,
      threadId,
      resourceId,
      tools: Object.keys(tools)
    });
    let schema = void 0;
    if (experimental_output) {
      this.logger.debug("[LLM] - Using experimental output", {
        runId
      });
      if (chunkPPEP5ZHX_cjs.isZodType(experimental_output)) {
        schema = experimental_output;
        if (chunkVW7YQWDW_cjs.isZodArray(schema)) {
          schema = chunkVW7YQWDW_cjs.getZodDef(schema).type;
        }
        const jsonSchemaToUse = zodToJson.zodToJsonSchema(schema, "jsonSchema7");
        schema = schemaCompat.jsonSchema(jsonSchemaToUse);
      } else {
        schema = schemaCompat.jsonSchema(experimental_output);
      }
    }
    const llmSpan = tracingContext.currentSpan?.createChildSpan({
      name: `llm: '${model.modelId}'`,
      type: "model_generation" /* MODEL_GENERATION */,
      input: {
        messages,
        schema
      },
      attributes: {
        model: model.modelId,
        provider: model.provider,
        parameters: {
          temperature,
          maxOutputTokens: rest.maxTokens,
          topP: rest.topP,
          frequencyPenalty: rest.frequencyPenalty,
          presencePenalty: rest.presencePenalty
        },
        streaming: false
      },
      metadata: {
        runId,
        threadId,
        resourceId
      },
      tracingPolicy: this.#options?.tracingPolicy
    });
    const argsForExecute = {
      ...rest,
      messages,
      model,
      temperature,
      tools: {
        ...tools
      },
      toolChoice,
      maxSteps,
      onStepFinish: async (props) => {
        try {
          await onStepFinish?.({ ...props, runId });
        } catch (e) {
          const mastraError = new chunk4U7ZLI36_cjs.MastraError(
            {
              id: "LLM_TEXT_ON_STEP_FINISH_CALLBACK_EXECUTION_FAILED",
              domain: "LLM" /* LLM */,
              category: "USER" /* USER */,
              details: {
                modelId: model.modelId,
                modelProvider: model.provider,
                runId: runId ?? "unknown",
                threadId: threadId ?? "unknown",
                resourceId: resourceId ?? "unknown",
                finishReason: props?.finishReason,
                toolCalls: props?.toolCalls ? JSON.stringify(props.toolCalls) : "",
                toolResults: props?.toolResults ? JSON.stringify(props.toolResults) : "",
                usage: props?.usage ? JSON.stringify(props.usage) : ""
              }
            },
            e
          );
          throw mastraError;
        }
        this.logger.debug("[LLM] - Text Step Change:", {
          text: props?.text,
          toolCalls: props?.toolCalls,
          toolResults: props?.toolResults,
          finishReason: props?.finishReason,
          usage: props?.usage,
          runId
        });
        const remainingTokens = parseInt(props?.response?.headers?.["x-ratelimit-remaining-tokens"] ?? "", 10);
        if (!isNaN(remainingTokens) && remainingTokens > 0 && remainingTokens < 2e3) {
          this.logger.warn("Rate limit approaching, waiting 10 seconds", { runId });
          await chunkPPEP5ZHX_cjs.delay(10 * 1e3);
        }
      },
      experimental_output: schema ? chunkHQHGGAB7_cjs.output_exports.object({
        schema
      }) : void 0
    };
    try {
      const result = await chunkS65YV6YB_cjs.executeWithContext({
        span: llmSpan,
        fn: () => chunkHQHGGAB7_cjs.generateText(argsForExecute)
      });
      if (schema && result.finishReason === "stop") {
        result.object = result.experimental_output;
      }
      llmSpan?.end({
        output: {
          text: result.text,
          object: result.object,
          reasoning: result.reasoningDetails,
          reasoningText: result.reasoning,
          files: result.files,
          sources: result.sources,
          warnings: result.warnings
        },
        attributes: {
          finishReason: result.finishReason,
          usage: convertV4Usage(result.usage)
        }
      });
      return result;
    } catch (e) {
      const mastraError = new chunk4U7ZLI36_cjs.MastraError(
        {
          id: "LLM_GENERATE_TEXT_AI_SDK_EXECUTION_FAILED",
          domain: "LLM" /* LLM */,
          category: "THIRD_PARTY" /* THIRD_PARTY */,
          details: {
            modelId: model.modelId,
            modelProvider: model.provider,
            runId: runId ?? "unknown",
            threadId: threadId ?? "unknown",
            resourceId: resourceId ?? "unknown"
          }
        },
        e
      );
      llmSpan?.error({ error: mastraError });
      throw mastraError;
    }
  }
  async __textObject({
    messages,
    structuredOutput,
    runId,
    threadId,
    resourceId,
    requestContext,
    tracingContext,
    ...rest
  }) {
    const model = this.#model;
    this.logger.debug(`[LLM] - Generating a text object`, { runId });
    const llmSpan = tracingContext.currentSpan?.createChildSpan({
      name: `llm: '${model.modelId}'`,
      type: "model_generation" /* MODEL_GENERATION */,
      input: {
        messages
      },
      attributes: {
        model: model.modelId,
        provider: model.provider,
        parameters: {
          temperature: rest.temperature,
          maxOutputTokens: rest.maxTokens,
          topP: rest.topP,
          frequencyPenalty: rest.frequencyPenalty,
          presencePenalty: rest.presencePenalty
        },
        streaming: false
      },
      metadata: {
        runId,
        threadId,
        resourceId
      },
      tracingPolicy: this.#options?.tracingPolicy
    });
    try {
      let output = "object";
      if (chunkVW7YQWDW_cjs.isZodArray(structuredOutput)) {
        output = "array";
        structuredOutput = chunkVW7YQWDW_cjs.getZodDef(structuredOutput).type;
      }
      const processedSchema = this._applySchemaCompat(structuredOutput);
      llmSpan?.update({
        input: {
          messages,
          schema: processedSchema
        }
      });
      const argsForExecute = {
        ...rest,
        messages,
        model,
        // @ts-expect-error - output in our implementation can only be object or array
        output,
        schema: processedSchema
      };
      try {
        const result = await chunkHQHGGAB7_cjs.generateObject(argsForExecute);
        llmSpan?.end({
          output: {
            object: result.object,
            warnings: result.warnings
          },
          attributes: {
            finishReason: result.finishReason,
            usage: convertV4Usage(result.usage)
          }
        });
        return result;
      } catch (e) {
        const mastraError = new chunk4U7ZLI36_cjs.MastraError(
          {
            id: "LLM_GENERATE_OBJECT_AI_SDK_EXECUTION_FAILED",
            domain: "LLM" /* LLM */,
            category: "THIRD_PARTY" /* THIRD_PARTY */,
            details: {
              modelId: model.modelId,
              modelProvider: model.provider,
              runId: runId ?? "unknown",
              threadId: threadId ?? "unknown",
              resourceId: resourceId ?? "unknown"
            }
          },
          e
        );
        llmSpan?.error({ error: mastraError });
        throw mastraError;
      }
    } catch (e) {
      if (e instanceof chunk4U7ZLI36_cjs.MastraError) {
        throw e;
      }
      const mastraError = new chunk4U7ZLI36_cjs.MastraError(
        {
          id: "LLM_GENERATE_OBJECT_AI_SDK_SCHEMA_CONVERSION_FAILED",
          domain: "LLM" /* LLM */,
          category: "USER" /* USER */,
          details: {
            modelId: model.modelId,
            modelProvider: model.provider,
            runId: runId ?? "unknown",
            threadId: threadId ?? "unknown",
            resourceId: resourceId ?? "unknown"
          }
        },
        e
      );
      llmSpan?.error({ error: mastraError });
      throw mastraError;
    }
  }
  __stream({
    messages,
    onStepFinish,
    onFinish,
    maxSteps = 5,
    tools = {},
    runId,
    temperature,
    toolChoice = "auto",
    experimental_output,
    threadId,
    resourceId,
    requestContext,
    tracingContext,
    ...rest
  }) {
    const model = this.#model;
    this.logger.debug(`[LLM] - Streaming text`, {
      runId,
      threadId,
      resourceId,
      messages,
      maxSteps,
      tools: Object.keys(tools || {})
    });
    let schema;
    if (experimental_output) {
      this.logger.debug("[LLM] - Using experimental output", {
        runId
      });
      if (typeof experimental_output.parse === "function") {
        schema = experimental_output;
        if (chunkVW7YQWDW_cjs.isZodArray(schema)) {
          schema = chunkVW7YQWDW_cjs.getZodDef(schema).type;
        }
      } else {
        schema = schemaCompat.jsonSchema(experimental_output);
      }
    }
    const llmSpan = tracingContext.currentSpan?.createChildSpan({
      name: `llm: '${model.modelId}'`,
      type: "model_generation" /* MODEL_GENERATION */,
      input: {
        messages
      },
      attributes: {
        model: model.modelId,
        provider: model.provider,
        parameters: {
          temperature,
          maxOutputTokens: rest.maxTokens,
          topP: rest.topP,
          frequencyPenalty: rest.frequencyPenalty,
          presencePenalty: rest.presencePenalty
        },
        streaming: true
      },
      metadata: {
        runId,
        threadId,
        resourceId
      },
      tracingPolicy: this.#options?.tracingPolicy
    });
    const argsForExecute = {
      model,
      temperature,
      tools: {
        ...tools
      },
      maxSteps,
      toolChoice,
      onStepFinish: async (props) => {
        try {
          await onStepFinish?.({ ...props, runId });
        } catch (e) {
          const mastraError = new chunk4U7ZLI36_cjs.MastraError(
            {
              id: "LLM_STREAM_ON_STEP_FINISH_CALLBACK_EXECUTION_FAILED",
              domain: "LLM" /* LLM */,
              category: "USER" /* USER */,
              details: {
                modelId: model.modelId,
                modelProvider: model.provider,
                runId: runId ?? "unknown",
                threadId: threadId ?? "unknown",
                resourceId: resourceId ?? "unknown",
                finishReason: props?.finishReason,
                toolCalls: props?.toolCalls ? JSON.stringify(props.toolCalls) : "",
                toolResults: props?.toolResults ? JSON.stringify(props.toolResults) : "",
                usage: props?.usage ? JSON.stringify(props.usage) : ""
              }
            },
            e
          );
          this.logger.trackException(mastraError);
          llmSpan?.error({ error: mastraError });
          throw mastraError;
        }
        this.logger.debug("[LLM] - Stream Step Change:", {
          text: props?.text,
          toolCalls: props?.toolCalls,
          toolResults: props?.toolResults,
          finishReason: props?.finishReason,
          usage: props?.usage,
          runId
        });
        const remainingTokens = parseInt(props?.response?.headers?.["x-ratelimit-remaining-tokens"] ?? "", 10);
        if (!isNaN(remainingTokens) && remainingTokens > 0 && remainingTokens < 2e3) {
          this.logger.warn("Rate limit approaching, waiting 10 seconds", { runId });
          await chunkPPEP5ZHX_cjs.delay(10 * 1e3);
        }
      },
      onFinish: async (props) => {
        llmSpan?.end({
          output: {
            text: props?.text,
            reasoning: props?.reasoningDetails,
            reasoningText: props?.reasoning,
            files: props?.files,
            sources: props?.sources,
            warnings: props?.warnings
          },
          attributes: {
            finishReason: props?.finishReason,
            usage: convertV4Usage(props?.usage)
          }
        });
        try {
          await onFinish?.({ ...props, runId });
        } catch (e) {
          const mastraError = new chunk4U7ZLI36_cjs.MastraError(
            {
              id: "LLM_STREAM_ON_FINISH_CALLBACK_EXECUTION_FAILED",
              domain: "LLM" /* LLM */,
              category: "USER" /* USER */,
              details: {
                modelId: model.modelId,
                modelProvider: model.provider,
                runId: runId ?? "unknown",
                threadId: threadId ?? "unknown",
                resourceId: resourceId ?? "unknown",
                finishReason: props?.finishReason,
                toolCalls: props?.toolCalls ? JSON.stringify(props.toolCalls) : "",
                toolResults: props?.toolResults ? JSON.stringify(props.toolResults) : "",
                usage: props?.usage ? JSON.stringify(props.usage) : ""
              }
            },
            e
          );
          llmSpan?.error({ error: mastraError });
          this.logger.trackException(mastraError);
          throw mastraError;
        }
        this.logger.debug("[LLM] - Stream Finished:", {
          text: props?.text,
          toolCalls: props?.toolCalls,
          toolResults: props?.toolResults,
          finishReason: props?.finishReason,
          usage: props?.usage,
          runId,
          threadId,
          resourceId
        });
      },
      ...rest,
      messages,
      experimental_output: schema ? chunkHQHGGAB7_cjs.output_exports.object({
        schema
      }) : void 0
    };
    try {
      return chunkS65YV6YB_cjs.executeWithContextSync({ span: llmSpan, fn: () => chunkHQHGGAB7_cjs.streamText(argsForExecute) });
    } catch (e) {
      const mastraError = new chunk4U7ZLI36_cjs.MastraError(
        {
          id: "LLM_STREAM_TEXT_AI_SDK_EXECUTION_FAILED",
          domain: "LLM" /* LLM */,
          category: "THIRD_PARTY" /* THIRD_PARTY */,
          details: {
            modelId: model.modelId,
            modelProvider: model.provider,
            runId: runId ?? "unknown",
            threadId: threadId ?? "unknown",
            resourceId: resourceId ?? "unknown"
          }
        },
        e
      );
      llmSpan?.error({ error: mastraError });
      throw mastraError;
    }
  }
  __streamObject({
    messages,
    runId,
    requestContext,
    threadId,
    resourceId,
    onFinish,
    structuredOutput,
    tracingContext,
    ...rest
  }) {
    const model = this.#model;
    this.logger.debug(`[LLM] - Streaming structured output`, {
      runId,
      messages
    });
    const llmSpan = tracingContext.currentSpan?.createChildSpan({
      name: `llm: '${model.modelId}'`,
      type: "model_generation" /* MODEL_GENERATION */,
      input: {
        messages
      },
      attributes: {
        model: model.modelId,
        provider: model.provider,
        parameters: {
          temperature: rest.temperature,
          maxOutputTokens: rest.maxTokens,
          topP: rest.topP,
          frequencyPenalty: rest.frequencyPenalty,
          presencePenalty: rest.presencePenalty
        },
        streaming: true
      },
      metadata: {
        runId,
        threadId,
        resourceId
      },
      tracingPolicy: this.#options?.tracingPolicy
    });
    try {
      let output = "object";
      if (chunkVW7YQWDW_cjs.isZodArray(structuredOutput)) {
        output = "array";
        structuredOutput = chunkVW7YQWDW_cjs.getZodDef(structuredOutput).type;
      }
      const processedSchema = this._applySchemaCompat(structuredOutput);
      llmSpan?.update({
        input: {
          messages,
          schema: processedSchema
        }
      });
      const argsForExecute = {
        ...rest,
        model,
        onFinish: async (props) => {
          llmSpan?.end({
            output: {
              text: props?.text,
              object: props?.object,
              reasoning: props?.reasoningDetails,
              reasoningText: props?.reasoning,
              files: props?.files,
              sources: props?.sources,
              warnings: props?.warnings
            },
            attributes: {
              finishReason: props?.finishReason,
              usage: props?.usage
            }
          });
          try {
            await onFinish?.({ ...props, runId });
          } catch (e) {
            const mastraError = new chunk4U7ZLI36_cjs.MastraError(
              {
                id: "LLM_STREAM_OBJECT_ON_FINISH_CALLBACK_EXECUTION_FAILED",
                domain: "LLM" /* LLM */,
                category: "USER" /* USER */,
                details: {
                  modelId: model.modelId,
                  modelProvider: model.provider,
                  runId: runId ?? "unknown",
                  threadId: threadId ?? "unknown",
                  resourceId: resourceId ?? "unknown",
                  toolCalls: "",
                  toolResults: "",
                  finishReason: "",
                  usage: props?.usage ? JSON.stringify(props.usage) : ""
                }
              },
              e
            );
            this.logger.trackException(mastraError);
            llmSpan?.error({ error: mastraError });
            throw mastraError;
          }
          this.logger.debug("[LLM] - Object Stream Finished:", {
            usage: props?.usage,
            runId,
            threadId,
            resourceId
          });
        },
        messages,
        // @ts-expect-error - output in our implementation can only be object or array
        output,
        schema: processedSchema
      };
      try {
        return chunkHQHGGAB7_cjs.streamObject(argsForExecute);
      } catch (e) {
        const mastraError = new chunk4U7ZLI36_cjs.MastraError(
          {
            id: "LLM_STREAM_OBJECT_AI_SDK_EXECUTION_FAILED",
            domain: "LLM" /* LLM */,
            category: "THIRD_PARTY" /* THIRD_PARTY */,
            details: {
              modelId: model.modelId,
              modelProvider: model.provider,
              runId: runId ?? "unknown",
              threadId: threadId ?? "unknown",
              resourceId: resourceId ?? "unknown"
            }
          },
          e
        );
        llmSpan?.error({ error: mastraError });
        throw mastraError;
      }
    } catch (e) {
      if (e instanceof chunk4U7ZLI36_cjs.MastraError) {
        llmSpan?.error({ error: e });
        throw e;
      }
      const mastraError = new chunk4U7ZLI36_cjs.MastraError(
        {
          id: "LLM_STREAM_OBJECT_AI_SDK_SCHEMA_CONVERSION_FAILED",
          domain: "LLM" /* LLM */,
          category: "USER" /* USER */,
          details: {
            modelId: model.modelId,
            modelProvider: model.provider,
            runId: runId ?? "unknown",
            threadId: threadId ?? "unknown",
            resourceId: resourceId ?? "unknown"
          }
        },
        e
      );
      llmSpan?.error({ error: mastraError });
      throw mastraError;
    }
  }
  convertToMessages(messages) {
    if (Array.isArray(messages)) {
      return messages.map((m) => {
        if (typeof m === "string") {
          return {
            role: "user",
            content: m
          };
        }
        return m;
      });
    }
    return [
      {
        role: "user",
        content: messages
      }
    ];
  }
  async generate(messages, {
    output,
    ...rest
  }) {
    const msgs = this.convertToMessages(messages);
    if (!output) {
      const { maxSteps, onStepFinish, ...textOptions } = rest;
      return await this.__text({
        messages: msgs,
        maxSteps,
        onStepFinish,
        ...textOptions
      });
    }
    return await this.__textObject({
      messages: msgs,
      structuredOutput: output,
      ...rest
    });
  }
  stream(messages, {
    maxSteps = 5,
    output,
    onFinish,
    ...rest
  }) {
    const msgs = this.convertToMessages(messages);
    if (!output) {
      return this.__stream({
        messages: msgs,
        maxSteps,
        onFinish,
        ...rest
      });
    }
    return this.__streamObject({
      messages: msgs,
      structuredOutput: output,
      onFinish,
      ...rest
    });
  }
};
function createStreamFromGenerateResult(result) {
  return new ReadableStream({
    start(controller) {
      controller.enqueue({ type: "stream-start", warnings: result.warnings });
      controller.enqueue({
        type: "response-metadata",
        id: result.response?.id,
        modelId: result.response?.modelId,
        timestamp: result.response?.timestamp
      });
      for (const message of result.content) {
        if (message.type === "tool-call") {
          const toolCall = message;
          controller.enqueue({
            type: "tool-input-start",
            id: toolCall.toolCallId,
            toolName: toolCall.toolName
          });
          controller.enqueue({
            type: "tool-input-delta",
            id: toolCall.toolCallId,
            delta: toolCall.input
          });
          controller.enqueue({
            type: "tool-input-end",
            id: toolCall.toolCallId
          });
          controller.enqueue(toolCall);
        } else if (message.type === "tool-result") {
          controller.enqueue(message);
        } else if (message.type === "text") {
          const text = message;
          const id = `msg_${crypto.randomUUID()}`;
          controller.enqueue({
            type: "text-start",
            id,
            providerMetadata: text.providerMetadata
          });
          controller.enqueue({
            type: "text-delta",
            id,
            delta: text.text
          });
          controller.enqueue({
            type: "text-end",
            id
          });
        } else if (message.type === "reasoning") {
          const id = `reasoning_${crypto.randomUUID()}`;
          const reasoning = message;
          controller.enqueue({
            type: "reasoning-start",
            id,
            providerMetadata: reasoning.providerMetadata
          });
          controller.enqueue({
            type: "reasoning-delta",
            id,
            delta: reasoning.text,
            providerMetadata: reasoning.providerMetadata
          });
          controller.enqueue({
            type: "reasoning-end",
            id,
            providerMetadata: reasoning.providerMetadata
          });
        } else if (message.type === "file") {
          const file = message;
          controller.enqueue({
            type: "file",
            mediaType: file.mediaType,
            data: file.data
          });
        } else if (message.type === "source") {
          const source = message;
          if (source.sourceType === "url") {
            controller.enqueue({
              type: "source",
              id: source.id,
              sourceType: "url",
              url: source.url,
              title: source.title,
              providerMetadata: source.providerMetadata
            });
          } else {
            controller.enqueue({
              type: "source",
              id: source.id,
              sourceType: "document",
              mediaType: source.mediaType,
              filename: source.filename,
              title: source.title,
              providerMetadata: source.providerMetadata
            });
          }
        }
      }
      controller.enqueue({
        type: "finish",
        finishReason: result.finishReason,
        usage: result.usage,
        providerMetadata: result.providerMetadata
      });
      controller.close();
    }
  });
}

// src/llm/model/aisdk/v5/model.ts
var AISDKV5LanguageModel = class {
  /**
   * The language model must specify which language model interface version it implements.
   */
  specificationVersion = "v2";
  /**
   * Name of the provider for logging purposes.
   */
  provider;
  /**
   * Provider-specific model ID for logging purposes.
   */
  modelId;
  /**
   * Supported URL patterns by media type for the provider.
   *
   * The keys are media type patterns or full media types (e.g. `*\/*` for everything, `audio/*`, `video/*`, or `application/pdf`).
   * and the values are arrays of regular expressions that match the URL paths.
   * The matching should be against lower-case URLs.
   * Matched URLs are supported natively by the model and are not downloaded.
   * @returns A map of supported URL patterns by media type (as a promise or a plain object).
   */
  supportedUrls;
  #model;
  constructor(config) {
    this.#model = config;
    this.provider = this.#model.provider;
    this.modelId = this.#model.modelId;
    this.supportedUrls = this.#model.supportedUrls;
  }
  async doGenerate(options) {
    const result = await this.#model.doGenerate(options);
    return {
      request: result.request,
      response: result.response,
      stream: createStreamFromGenerateResult(result)
    };
  }
  async doStream(options) {
    return await this.#model.doStream(options);
  }
};

exports.AISDKV5LanguageModel = AISDKV5LanguageModel;
exports.MastraLLMV1 = MastraLLMV1;
exports.createStreamFromGenerateResult = createStreamFromGenerateResult;
//# sourceMappingURL=chunk-HTNZMI2D.cjs.map
//# sourceMappingURL=chunk-HTNZMI2D.cjs.map