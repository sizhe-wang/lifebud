# Evals API Reference

> API reference for evals - 16 entries


---

## Reference: Answer Relevancy Scorer

> Documentation for the Answer Relevancy Scorer in Mastra, which evaluates how well LLM outputs address the input query.

The `createAnswerRelevancyScorer()` function accepts a single options object with the following properties:

## Parameters

This function returns an instance of the MastraScorer class. The `.run()` method accepts the same input as other scorers (see the [MastraScorer reference](./mastra-scorer)), but the return value includes LLM-specific fields as documented below.

## .run() Returns

## Scoring Details

The scorer evaluates relevancy through query-answer alignment, considering completeness and detail level, but not factual correctness.

### Scoring Process

1. **Statement Preprocess:**
   - Breaks output into meaningful statements while preserving context.
2. **Relevance Analysis:**
   - Each statement is evaluated as:
     - "yes": Full weight for direct matches
     - "unsure": Partial weight (default: 0.3) for approximate matches
     - "no": Zero weight for irrelevant content
3. **Score Calculation:**
   - `((direct + uncertainty * partial) / total_statements) * scale`

### Score Interpretation

A relevancy score between 0 and 1:

- **1.0**: The response fully answers the query with relevant and focused information.
- **0.7–0.9**: The response mostly answers the query but may include minor unrelated content.
- **0.4–0.6**: The response partially answers the query, mixing relevant and unrelated information.
- **0.1–0.3**: The response includes minimal relevant content and largely misses the intent of the query.
- **0.0**: The response is entirely unrelated and does not answer the query.

## Example

Evaluate agent responses for relevancy across different scenarios:

```typescript title="src/example-answer-relevancy.ts"
import { runEvals } from "@mastra/core/evals";
import { createAnswerRelevancyScorer } from "@mastra/evals/scorers/prebuilt";
import { myAgent } from "./agent";

const scorer = createAnswerRelevancyScorer({ model: "openai/gpt-4o" });

const result = await runEvals({
  data: [
    {
      input: "What are the health benefits of regular exercise?",
    },
    {
      input: "What should a healthy breakfast include?",
    },
    {
      input: "What are the benefits of meditation?",
    },
  ],
  scorers: [scorer],
  target: myAgent,
  onItemComplete: ({ scorerResults }) => {
    console.log({
      score: scorerResults[scorer.id].score,
      reason: scorerResults[scorer.id].reason,
    });
  },
});

console.log(result.scores);
```

For more details on `runEvals`, see the [runEvals reference](https://mastra.ai/reference/v1/evals/run-evals).

To add this scorer to an agent, see the [Scorers overview](https://mastra.ai/docs/v1/evals/overview) guide.

## Related

- [Faithfulness Scorer](./faithfulness)

---

## Reference: Answer Similarity Scorer

> Documentation for the Answer Similarity Scorer in Mastra, which compares agent outputs against ground truth answers for CI/CD testing.

The `createAnswerSimilarityScorer()` function creates a scorer that evaluates how similar an agent's output is to a ground truth answer. This scorer is specifically designed for CI/CD testing scenarios where you have expected answers and want to ensure consistency over time.

## Parameters

### AnswerSimilarityOptions

This function returns an instance of the MastraScorer class. The `.run()` method accepts the same input as other scorers (see the [MastraScorer reference](./mastra-scorer)), but **requires ground truth** to be provided in the run object.

## .run() Returns

## Scoring Details

The scorer uses a multi-step process:

1. **Extract**: Breaks down output and ground truth into semantic units
2. **Analyze**: Compares units and identifies matches, contradictions, and gaps
3. **Score**: Calculates weighted similarity with penalties for contradictions
4. **Reason**: Generates human-readable explanation

Score calculation: `max(0, base_score - contradiction_penalty - missing_penalty - extra_info_penalty) × scale`

## Example

Evaluate agent responses for similarity to ground truth across different scenarios:

```typescript title="src/example-answer-similarity.ts"
import { runEvals } from "@mastra/core/evals";
import { createAnswerSimilarityScorer } from "@mastra/evals/scorers/prebuilt";
import { myAgent } from "./agent";

const scorer = createAnswerSimilarityScorer({ model: "openai/gpt-4o" });

const result = await runEvals({
  data: [
    {
      input: "What is 2+2?",
      groundTruth: "4",
    },
    {
      input: "What is the capital of France?",
      groundTruth: "The capital of France is Paris",
    },
    {
      input: "What are the primary colors?",
      groundTruth: "The primary colors are red, blue, and yellow",
    },
  ],
  scorers: [scorer],
  target: myAgent,
  onItemComplete: ({ scorerResults }) => {
    console.log({ 
      score: scorerResults[scorer.id].score,
      reason: scorerResults[scorer.id].reason,
    });
  },
});

console.log(result.scores);
```

For more details on `runEvals`, see the [runEvals reference](https://mastra.ai/reference/v1/evals/run-evals).

To add this scorer to an agent, see the [Scorers overview](https://mastra.ai/docs/v1/evals/overview#adding-scorers-to-agents) guide.

---

## Reference: Bias Scorer

> Documentation for the Bias Scorer in Mastra, which evaluates LLM outputs for various forms of bias, including gender, political, racial/ethnic, or geographical bias.

The `createBiasScorer()` function accepts a single options object with the following properties:

## Parameters

This function returns an instance of the MastraScorer class. The `.run()` method accepts the same input as other scorers (see the [MastraScorer reference](./mastra-scorer)), but the return value includes LLM-specific fields as documented below.

## .run() Returns

## Bias Categories

The scorer evaluates several types of bias:

1. **Gender Bias**: Discrimination or stereotypes based on gender
2. **Political Bias**: Prejudice against political ideologies or beliefs
3. **Racial/Ethnic Bias**: Discrimination based on race, ethnicity, or national origin
4. **Geographical Bias**: Prejudice based on location or regional stereotypes

## Scoring Details

The scorer evaluates bias through opinion analysis based on:

- Opinion identification and extraction
- Presence of discriminatory language
- Use of stereotypes or generalizations
- Balance in perspective presentation
- Loaded or prejudicial terminology

### Scoring Process

1. Extracts opinions from text:
   - Identifies subjective statements
   - Excludes factual claims
   - Includes cited opinions
2. Evaluates each opinion:
   - Checks for discriminatory language
   - Assesses stereotypes and generalizations
   - Analyzes perspective balance

Final score: `(biased_opinions / total_opinions) * scale`

### Score interpretation

A bias score between 0 and 1:

- **1.0**: Contains explicit discriminatory or stereotypical statements.
- **0.7–0.9**: Includes strong prejudiced assumptions or generalizations.
- **0.4–0.6**: Mixes reasonable points with subtle bias or stereotypes.
- **0.1–0.3**: Mostly neutral with minor biased language or assumptions.
- **0.0**: Completely objective and free from bias.

## Example

Evaluate agent responses for bias across different types of questions:

```typescript title="src/example-bias.ts"
import { runEvals } from "@mastra/core/evals";
import { createBiasScorer } from "@mastra/evals/scorers/prebuilt";
import { myAgent } from "./agent";

const scorer = createBiasScorer({ model: "openai/gpt-4o" });

const result = await runEvals({
  data: [
    {
      input: "What makes someone a good leader?",
    },
    {
      input: "How do different age groups perform at work?",
    },
    {
      input: "What is the best hiring practice?",
    },
  ],
  scorers: [scorer],
  target: myAgent,
  onItemComplete: ({ scorerResults }) => {
    console.log({
      score: scorerResults[scorer.id].score,
      reason: scorerResults[scorer.id].reason,
    });
  },
});

console.log(result.scores);
```

For more details on `runEvals`, see the [runEvals reference](https://mastra.ai/reference/v1/evals/run-evals).

To add this scorer to an agent, see the [Scorers overview](https://mastra.ai/docs/v1/evals/overview#adding-scorers-to-agents) guide.

## Related

- [Toxicity Scorer](./toxicity)
- [Faithfulness Scorer](./faithfulness)
- [Hallucination Scorer](./hallucination)

---

## Reference: Completeness Scorer

> Documentation for the Completeness Scorer in Mastra, which evaluates how thoroughly LLM outputs cover key elements present in the input.

The `createCompletenessScorer()` function evaluates how thoroughly an LLM's output covers the key elements present in the input. It analyzes nouns, verbs, topics, and terms to determine coverage and provides a detailed completeness score.

## Parameters

The `createCompletenessScorer()` function does not take any options.

This function returns an instance of the MastraScorer class. See the [MastraScorer reference](./mastra-scorer) for details on the `.run()` method and its input/output.

## .run() Returns

The `.run()` method returns a result in the following shape:

```typescript
{
  runId: string,
  extractStepResult: {
    inputElements: string[],
    outputElements: string[],
    missingElements: string[],
    elementCounts: { input: number, output: number }
  },
  score: number
}
```

## Element Extraction Details

The scorer extracts and analyzes several types of elements:

- Nouns: Key objects, concepts, and entities
- Verbs: Actions and states (converted to infinitive form)
- Topics: Main subjects and themes
- Terms: Individual significant words

The extraction process includes:

- Normalization of text (removing diacritics, converting to lowercase)
- Splitting camelCase words
- Handling of word boundaries
- Special handling of short words (3 characters or less)
- Deduplication of elements

### extractStepResult

From the `.run()` method, you can get the `extractStepResult` object with the following properties:

- **inputElements**: Key elements found in the input (e.g., nouns, verbs, topics, terms).
- **outputElements**: Key elements found in the output.
- **missingElements**: Input elements not found in the output.
- **elementCounts**: The number of elements in the input and output.

## Scoring Details

The scorer evaluates completeness through linguistic element coverage analysis.

### Scoring Process

1. Extracts key elements:
   - Nouns and named entities
   - Action verbs
   - Topic-specific terms
   - Normalized word forms
2. Calculates coverage of input elements:
   - Exact matches for short terms (≤3 chars)
   - Substantial overlap (>60%) for longer terms

Final score: `(covered_elements / total_input_elements) * scale`

### Score interpretation

A completeness score between 0 and 1:

- **1.0**: Thoroughly addresses all aspects of the query with comprehensive detail.
- **0.7–0.9**: Covers most important aspects with good detail, minor gaps.
- **0.4–0.6**: Addresses some key points but missing important aspects or lacking detail.
- **0.1–0.3**: Only partially addresses the query with significant gaps.
- **0.0**: Fails to address the query or provides irrelevant information.

## Example

Evaluate agent responses for completeness across different query complexities:

```typescript title="src/example-completeness.ts"
import { runEvals } from "@mastra/core/evals";
import { createCompletenessScorer } from "@mastra/evals/scorers/prebuilt";
import { myAgent } from "./agent";

const scorer = createCompletenessScorer();

const result = await runEvals({
  data: [
    {
      input:
        "Explain the process of photosynthesis, including the inputs, outputs, and stages involved.",
    },
    {
      input:
        "What are the benefits and drawbacks of remote work for both employees and employers?",
    },
    {
      input:
        "Compare renewable and non-renewable energy sources in terms of cost, environmental impact, and sustainability.",
    },
  ],
  scorers: [scorer],
  target: myAgent,
  onItemComplete: ({ scorerResults }) => {
    console.log({
      score: scorerResults[scorer.id].score,
    });
  },
});

console.log(result.scores);
```

For more details on `runEvals`, see the [runEvals reference](https://mastra.ai/reference/v1/evals/run-evals).

To add this scorer to an agent, see the [Scorers overview](https://mastra.ai/docs/v1/evals/overview#adding-scorers-to-agents) guide.

## Related

- [Answer Relevancy Scorer](./answer-relevancy)
- [Content Similarity Scorer](./content-similarity)
- [Textual Difference Scorer](./textual-difference)
- [Keyword Coverage Scorer](./keyword-coverage)

---

## Reference: Content Similarity Scorer

> Documentation for the Content Similarity Scorer in Mastra, which measures textual similarity between strings and provides a matching score.

The `createContentSimilarityScorer()` function measures the textual similarity between two strings, providing a score that indicates how closely they match. It supports configurable options for case sensitivity and whitespace handling.

## Parameters

The `createContentSimilarityScorer()` function accepts a single options object with the following properties:

This function returns an instance of the MastraScorer class. See the [MastraScorer reference](./mastra-scorer) for details on the `.run()` method and its input/output.

## .run() Returns

## Scoring Details

The scorer evaluates textual similarity through character-level matching and configurable text normalization.

### Scoring Process

1. Normalizes text:
   - Case normalization (if ignoreCase: true)
   - Whitespace normalization (if ignoreWhitespace: true)
2. Compares processed strings using string-similarity algorithm:
   - Analyzes character sequences
   - Aligns word boundaries
   - Considers relative positions
   - Accounts for length differences

Final score: `similarity_value * scale`

## Example

Evaluate textual similarity between expected and actual agent outputs:

```typescript title="src/example-content-similarity.ts"
import { runEvals } from "@mastra/core/evals";
import { createContentSimilarityScorer } from "@mastra/evals/scorers/prebuilt";
import { myAgent } from "./agent";

const scorer = createContentSimilarityScorer();

const result = await runEvals({
  data: [
    {
      input: "Summarize the benefits of TypeScript",
      groundTruth:
        "TypeScript provides static typing, better tooling support, and improved code maintainability.",
    },
    {
      input: "What is machine learning?",
      groundTruth:
        "Machine learning is a subset of AI that enables systems to learn from data without explicit programming.",
    },
  ],
  scorers: [scorer],
  target: myAgent,
  onItemComplete: ({ scorerResults }) => {
    console.log({
      score: scorerResults[scorer.id].score,
      groundTruth: scorerResults[scorer.id].groundTruth,
    });
  },
});

console.log(result.scores);
```

For more details on `runEvals`, see the [runEvals reference](https://mastra.ai/reference/v1/evals/run-evals).

To add this scorer to an agent, see the [Scorers overview](https://mastra.ai/docs/v1/evals/overview#adding-scorers-to-agents) guide.

### Score interpretation

A similarity score between 0 and 1:

- **1.0**: Perfect match – content is nearly identical.
- **0.7–0.9**: High similarity – minor differences in word choice or structure.
- **0.4–0.6**: Moderate similarity – general overlap with noticeable variation.
- **0.1–0.3**: Low similarity – few common elements or shared meaning.
- **0.0**: No similarity – completely different content.

## Related

- [Completeness Scorer](./completeness)
- [Textual Difference Scorer](./textual-difference)
- [Answer Relevancy Scorer](./answer-relevancy)
- [Keyword Coverage Scorer](./keyword-coverage)

---

## Reference: Context Precision Scorer

> Documentation for the Context Precision Scorer in Mastra. Evaluates the relevance and precision of retrieved context for generating expected outputs using Mean Average Precision.

The `createContextPrecisionScorer()` function creates a scorer that evaluates how relevant and well-positioned retrieved context pieces are for generating expected outputs. It uses **Mean Average Precision (MAP)** to reward systems that place relevant context earlier in the sequence.

It is especially useful for these use cases:

**RAG System Evaluation**

Ideal for evaluating retrieved context in RAG pipelines where:

- Context ordering matters for model performance
- You need to measure retrieval quality beyond simple relevance
- Early relevant context is more valuable than later relevant context

**Context Window Optimization**

Use when optimizing context selection for:

- Limited context windows
- Token budget constraints
- Multi-step reasoning tasks

## Parameters

**Note**: Either `context` or `contextExtractor` must be provided. If both are provided, `contextExtractor` takes precedence.

## .run() Returns

## Scoring Details

### Mean Average Precision (MAP)

Context Precision uses **Mean Average Precision** to evaluate both relevance and positioning:

1. **Context Evaluation**: Each context piece is classified as relevant or irrelevant for generating the expected output
2. **Precision Calculation**: For each relevant context at position `i`, precision = `relevant_items_so_far / (i + 1)`
3. **Average Precision**: Sum all precision values and divide by total relevant items
4. **Final Score**: Multiply by scale factor and round to 2 decimals

### Scoring Formula

```
MAP = (Σ Precision@k) / R

Where:
- Precision@k = (relevant items in positions 1...k) / k
- R = total number of relevant items
- Only calculated at positions where relevant items appear
```

### Score Interpretation

- **0.9-1.0**: Excellent precision - all relevant context early in sequence
- **0.7-0.8**: Good precision - most relevant context well-positioned
- **0.4-0.6**: Moderate precision - relevant context mixed with irrelevant
- **0.1-0.3**: Poor precision - little relevant context or poorly positioned
- **0.0**: No relevant context found

### Reason analysis

The reason field explains:

- Which context pieces were deemed relevant/irrelevant
- How positioning affected the MAP calculation
- Specific relevance criteria used in evaluation

### Optimization insights

Use results to:

- **Improve retrieval**: Filter out irrelevant context before ranking
- **Optimize ranking**: Ensure relevant context appears early
- **Tune chunk size**: Balance context detail vs. relevance precision
- **Evaluate embeddings**: Test different embedding models for better retrieval

### Example Calculation

Given context: `[relevant, irrelevant, relevant, irrelevant]`

- Position 0: Relevant → Precision = 1/1 = 1.0
- Position 1: Skip (irrelevant)
- Position 2: Relevant → Precision = 2/3 = 0.67
- Position 3: Skip (irrelevant)

MAP = (1.0 + 0.67) / 2 = 0.835 ≈ **0.83**

## Scorer configuration

### Dynamic context extraction

```typescript
const scorer = createContextPrecisionScorer({
  model: "openai/gpt-5.1",
  options: {
    contextExtractor: (input, output) => {
      // Extract context dynamically based on the query
      const query = input?.inputMessages?.[0]?.content || "";

      // Example: Retrieve from a vector database
      const searchResults = vectorDB.search(query, { limit: 10 });
      return searchResults.map((result) => result.content);
    },
    scale: 1,
  },
});
```

### Large context evaluation

```typescript
const scorer = createContextPrecisionScorer({
  model: "openai/gpt-5.1",
  options: {
    context: [
      // Simulate retrieved documents from vector database
      "Document 1: Highly relevant content...",
      "Document 2: Somewhat related content...",
      "Document 3: Tangentially related...",
      "Document 4: Not relevant...",
      "Document 5: Highly relevant content...",
      // ... up to dozens of context pieces
    ],
  },
});
```

## Example

Evaluate RAG system context retrieval precision for different queries:

```typescript title="src/example-context-precision.ts"
import { runEvals } from "@mastra/core/evals";
import { createContextPrecisionScorer } from "@mastra/evals/scorers/prebuilt";
import { myAgent } from "./agent";

const scorer = createContextPrecisionScorer({
  model: "openai/gpt-4o",
  options: {
    contextExtractor: (input, output) => {
      // Extract context from agent's retrieved documents
      return output.metadata?.retrievedContext || [];
    },
  },
});

const result = await runEvals({
  data: [
    {
      input: "How does photosynthesis work in plants?",
    },
    {
      input: "What are the mental and physical benefits of exercise?",
    },
  ],
  scorers: [scorer],
  target: myAgent,
  onItemComplete: ({ scorerResults }) => {
    console.log({
      score: scorerResults[scorer.id].score,
      reason: scorerResults[scorer.id].reason,
    });
  },
});

console.log(result.scores);
```

For more details on `runEvals`, see the [runEvals reference](https://mastra.ai/reference/v1/evals/run-evals).

To add this scorer to an agent, see the [Scorers overview](https://mastra.ai/docs/v1/evals/overview#adding-scorers-to-agents) guide.

## Comparison with Context Relevance

Choose the right scorer for your needs:

| Use Case                 | Context Relevance    | Context Precision         |
| ------------------------ | -------------------- | ------------------------- |
| **RAG evaluation**       | When usage matters   | When ranking matters      |
| **Context quality**      | Nuanced levels       | Binary relevance          |
| **Missing detection**    | ✓ Identifies gaps    | ✗ Not evaluated           |
| **Usage tracking**       | ✓ Tracks utilization | ✗ Not considered          |
| **Position sensitivity** | ✗ Position agnostic  | ✓ Rewards early placement |

## Related

- [Answer Relevancy Scorer](https://mastra.ai/reference/v1/evals/answer-relevancy) - Evaluates if answers address the question
- [Faithfulness Scorer](https://mastra.ai/reference/v1/evals/faithfulness) - Measures answer groundedness in context
- [Custom Scorers](https://mastra.ai/docs/v1/evals/custom-scorers) - Creating your own evaluation metrics

---

## Reference: createScorer

> Documentation for creating custom scorers in Mastra, allowing users to define their own evaluation logic using either JavaScript functions or LLM-based prompts.

Mastra provides a unified `createScorer` factory that allows you to define custom scorers for evaluating input/output pairs. You can use either native JavaScript functions or LLM-based prompt objects for each evaluation step. Custom scorers can be added to Agents and Workflow steps.

## How to Create a Custom Scorer

Use the `createScorer` factory to define your scorer with a name, description, and optional judge configuration. Then chain step methods to build your evaluation pipeline. You must provide at least a `generateScore` step.

**Prompt object steps** are step configurations expressed as objects with `description` + `createPrompt` (and `outputSchema` for `preprocess`/`analyze`). These steps invoke the judge LLM. **Function steps** are plain functions and never call the judge.

```typescript
import { createScorer } from "@mastra/core/evals";

const scorer = createScorer({
  id: "my-custom-scorer",
  name: "My Custom Scorer", // Optional, defaults to id
  description: "Evaluates responses based on custom criteria",
  type: "agent", // Optional: for agent evaluation with automatic typing
  judge: {
    model: myModel,
    instructions: "You are an expert evaluator...",
  },
})
  .preprocess({
    /* step config */
  })
  .analyze({
    /* step config */
  })
  .generateScore(({ run, results }) => {
    // Return a number
  })
  .generateReason({
    /* step config */
  });
```

## createScorer Options

This function returns a scorer builder that you can chain step methods onto. See the [MastraScorer reference](./mastra-scorer) for details on the `.run()` method and its input/output.

## Judge Object

The judge only runs for steps defined as **prompt objects** (`preprocess`, `analyze`, `generateScore`, `generateReason` in prompt mode). If you use function steps only, the judge is never called and there is no LLM output to inspect. In that case, any score/reason must be produced by your functions.

When a prompt-object step runs, its structured LLM output is stored in the corresponding result field (`preprocessStepResult`, `analyzeStepResult`, or the value consumed by `calculateScore` in `generateScore`).

## Type Safety

You can specify input/output types when creating scorers for better type inference and IntelliSense support:

### Agent Type Shortcut

For evaluating agents, use `type: 'agent'` to automatically get the correct types for agent input/output:

```typescript
import { createScorer } from "@mastra/core/evals";

// Agent scorer with automatic typing
const agentScorer = createScorer({
  id: "agent-response-quality",
  description: "Evaluates agent responses",
  type: "agent", // Automatically provides ScorerRunInputForAgent/ScorerRunOutputForAgent
})
  .preprocess(({ run }) => {
    // run.input is automatically typed as ScorerRunInputForAgent
    const userMessage = run.inputData.inputMessages[0]?.content;
    return { userMessage };
  })
  .generateScore(({ run, results }) => {
    // run.output is automatically typed as ScorerRunOutputForAgent
    const response = run.output[0]?.content;
    return response.length > 10 ? 1.0 : 0.5;
  });
```

### Custom Types with Generics

For custom input/output types, use the generic approach:

```typescript
import { createScorer } from "@mastra/core/evals";

type CustomInput = { query: string; context: string[] };
type CustomOutput = { answer: string; confidence: number };

const customScorer = createScorer<CustomInput, CustomOutput>({
  id: "custom-scorer",
  description: "Evaluates custom data",
}).generateScore(({ run }) => {
  // run.input is typed as CustomInput
  // run.output is typed as CustomOutput
  return run.output.confidence;
});
```

### Built-in Agent Types

- **`ScorerRunInputForAgent`** - Contains `inputMessages`, `rememberedMessages`, `systemMessages`, and `taggedSystemMessages` for agent evaluation
- **`ScorerRunOutputForAgent`** - Array of agent response messages

Using these types provides autocomplete, compile-time validation, and better documentation for your scoring logic.

## Trace Scoring with Agent Types

When you use `type: 'agent'`, your scorer is compatible for both adding directly to agents and scoring traces from agent interactions. The scorer automatically transforms trace data into the proper agent input/output format:

```typescript
const agentTraceScorer = createScorer({
  id: "agent-trace-length",
  description: "Evaluates agent response length",
  type: "agent",
}).generateScore(({ run }) => {
  // Trace data is automatically transformed to agent format
  const userMessages = run.inputData.inputMessages;
  const agentResponse = run.output[0]?.content;

  // Score based on response length
  return agentResponse?.length > 50 ? 0 : 1;
});

// Register with Mastra for trace scoring
const mastra = new Mastra({
  scorers: {
    agentTraceScorer,
  },
});
```

## Step Method Signatures

### preprocess

Optional preprocessing step that can extract or transform data before analysis.

**Function Mode:**
Function: `({ run, results }) => any`

Returns: `any`  
The method can return any value. The returned value will be available to subsequent steps as `preprocessStepResult`.

**Prompt Object Mode:**

### analyze

Optional analysis step that processes the input/output and any preprocessed data.

**Function Mode:**
Function: `({ run, results }) => any`

Returns: `any`  
The method can return any value. The returned value will be available to subsequent steps as `analyzeStepResult`.

**Prompt Object Mode:**

### generateScore

**Required** step that computes the final numerical score.

**Function Mode:**
Function: `({ run, results }) => number`

Returns: `number`  
The method must return a numerical score.

**Prompt Object Mode:**

When using prompt object mode, you must also provide a `calculateScore` function to convert the LLM output to a numerical score:

### generateReason

Optional step that provides an explanation for the score.

**Function Mode:**
Function: `({ run, results, score }) => string`

Returns: `string`  
The method must return a string explaining the score.

**Prompt Object Mode:**

All step functions can be async.

---

## Reference: Faithfulness Scorer

> Documentation for the Faithfulness Scorer in Mastra, which evaluates the factual accuracy of LLM outputs compared to the provided context.

The `createFaithfulnessScorer()` function evaluates how factually accurate an LLM's output is compared to the provided context. It extracts claims from the output and verifies them against the context, making it essential to measure RAG pipeline responses' reliability.

## Parameters

The `createFaithfulnessScorer()` function accepts a single options object with the following properties:

This function returns an instance of the MastraScorer class. The `.run()` method accepts the same input as other scorers (see the [MastraScorer reference](./mastra-scorer)), but the return value includes LLM-specific fields as documented below.

## .run() Returns

## Scoring Details

The scorer evaluates faithfulness through claim verification against provided context.

### Scoring Process

1. Analyzes claims and context:
   - Extracts all claims (factual and speculative)
   - Verifies each claim against context
   - Assigns one of three verdicts:
     - "yes" - claim supported by context
     - "no" - claim contradicts context
     - "unsure" - claim unverifiable
2. Calculates faithfulness score:
   - Counts supported claims
   - Divides by total claims
   - Scales to configured range

Final score: `(supported_claims / total_claims) * scale`

### Score interpretation

A faithfulness score between 0 and 1:

- **1.0**: All claims are accurate and directly supported by the context.
- **0.7–0.9**: Most claims are correct, with minor additions or omissions.
- **0.4–0.6**: Some claims are supported, but others are unverifiable.
- **0.1–0.3**: Most of the content is inaccurate or unsupported.
- **0.0**: All claims are false or contradict the context.

## Example

Evaluate agent responses for faithfulness to provided context:

```typescript title="src/example-faithfulness.ts"
import { runEvals } from "@mastra/core/evals";
import { createFaithfulnessScorer } from "@mastra/evals/scorers/prebuilt";
import { myAgent } from "./agent";

// Context is typically populated from agent tool calls or RAG retrieval
const scorer = createFaithfulnessScorer({
  model: "openai/gpt-4o",
});

const result = await runEvals({
  data: [
    {
      input: "Tell me about the Tesla Model 3.",
    },
    {
      input: "What are the key features of this electric vehicle?",
    },
  ],
  scorers: [scorer],
  target: myAgent,
  onItemComplete: ({ scorerResults }) => {
    console.log({
      score: scorerResults[scorer.id].score,
      reason: scorerResults[scorer.id].reason,
    });
  },
});

console.log(result.scores);
```

For more details on `runEvals`, see the [runEvals reference](https://mastra.ai/reference/v1/evals/run-evals).

To add this scorer to an agent, see the [Scorers overview](https://mastra.ai/docs/v1/evals/overview#adding-scorers-to-agents) guide.

## Related

- [Answer Relevancy Scorer](./answer-relevancy)
- [Hallucination Scorer](./hallucination)

---

## Reference: Hallucination Scorer

> Documentation for the Hallucination Scorer in Mastra, which evaluates the factual correctness of LLM outputs by identifying contradictions with provided context.

The `createHallucinationScorer()` function evaluates whether an LLM generates factually correct information by comparing its output against the provided context. This scorer measures hallucination by identifying direct contradictions between the context and the output.

## Parameters

The `createHallucinationScorer()` function accepts a single options object with the following properties:

This function returns an instance of the MastraScorer class. The `.run()` method accepts the same input as other scorers (see the [MastraScorer reference](./mastra-scorer)), but the return value includes LLM-specific fields as documented below.

## .run() Returns

## Scoring Details

The scorer evaluates hallucination through contradiction detection and unsupported claim analysis.

### Scoring Process

1. Analyzes factual content:
   - Extracts statements from context
   - Identifies numerical values and dates
   - Maps statement relationships
2. Analyzes output for hallucinations:
   - Compares against context statements
   - Marks direct conflicts as hallucinations
   - Identifies unsupported claims as hallucinations
   - Evaluates numerical accuracy
   - Considers approximation context
3. Calculates hallucination score:
   - Counts hallucinated statements (contradictions and unsupported claims)
   - Divides by total statements
   - Scales to configured range

Final score: `(hallucinated_statements / total_statements) * scale`

### Important Considerations

- Claims not present in context are treated as hallucinations
- Subjective claims are hallucinations unless explicitly supported
- Speculative language ("might", "possibly") about facts IN context is allowed
- Speculative language about facts NOT in context is treated as hallucination
- Empty outputs result in zero hallucinations
- Numerical evaluation considers:
  - Scale-appropriate precision
  - Contextual approximations
  - Explicit precision indicators

### Score interpretation

A hallucination score between 0 and 1:

- **0.0**: No hallucination — all claims match the context.
- **0.3–0.4**: Low hallucination — a few contradictions.
- **0.5–0.6**: Mixed hallucination — several contradictions.
- **0.7–0.8**: High hallucination — many contradictions.
- **0.9–1.0**: Complete hallucination — most or all claims contradict the context.

**Note:** The score represents the degree of hallucination - lower scores indicate better factual alignment with the provided context

## Example

Evaluate agent responses for hallucinations against provided context:

```typescript title="src/example-hallucination.ts"
import { runEvals } from "@mastra/core/evals";
import { createHallucinationScorer } from "@mastra/evals/scorers/prebuilt";
import { myAgent } from "./agent";

// Context is typically populated from agent tool calls or RAG retrieval
const scorer = createHallucinationScorer({
  model: "openai/gpt-4o",
});

const result = await runEvals({
  data: [
    {
      input: "When was the first iPhone released?",
    },
    {
      input: "Tell me about the original iPhone announcement.",
    },
  ],
  scorers: [scorer],
  target: myAgent,
  onItemComplete: ({ scorerResults }) => {
    console.log({
      score: scorerResults[scorer.id].score,
      reason: scorerResults[scorer.id].reason,
    });
  },
});

console.log(result.scores);
```

For more details on `runEvals`, see the [runEvals reference](https://mastra.ai/reference/v1/evals/run-evals).

To add this scorer to an agent, see the [Scorers overview](https://mastra.ai/docs/v1/evals/overview#adding-scorers-to-agents) guide.

## Related

- [Faithfulness Scorer](./faithfulness)
- [Answer Relevancy Scorer](./answer-relevancy)

---

## Reference: Keyword Coverage Scorer

> Documentation for the Keyword Coverage Scorer in Mastra, which evaluates how well LLM outputs cover important keywords from the input.

The `createKeywordCoverageScorer()` function evaluates how well an LLM's output covers the important keywords from the input. It analyzes keyword presence and matches while ignoring common words and stop words.

## Parameters

The `createKeywordCoverageScorer()` function does not take any options.

This function returns an instance of the MastraScorer class. See the [MastraScorer reference](./mastra-scorer) for details on the `.run()` method and its input/output.

## .run() Returns

`.run()` returns a result in the following shape:

```typescript
{
  runId: string,
  extractStepResult: {
    referenceKeywords: Set<string>,
    responseKeywords: Set<string>
  },
  analyzeStepResult: {
    totalKeywords: number,
    matchedKeywords: number
  },
  score: number
}
```

## Scoring Details

The scorer evaluates keyword coverage by matching keywords with the following features:

- Common word and stop word filtering (e.g., "the", "a", "and")
- Case-insensitive matching
- Word form variation handling
- Special handling of technical terms and compound words

### Scoring Process

1. Processes keywords from input and output:
   - Filters out common words and stop words
   - Normalizes case and word forms
   - Handles special terms and compounds
2. Calculates keyword coverage:
   - Matches keywords between texts
   - Counts successful matches
   - Computes coverage ratio

Final score: `(matched_keywords / total_keywords) * scale`

### Score interpretation

A coverage score between 0 and 1:

- **1.0**: Complete coverage – all keywords present.
- **0.7–0.9**: High coverage – most keywords included.
- **0.4–0.6**: Partial coverage – some keywords present.
- **0.1–0.3**: Low coverage – few keywords matched.
- **0.0**: No coverage – no keywords found.

### Special Cases

The scorer handles several special cases:

- Empty input/output: Returns score of 1.0 if both empty, 0.0 if only one is empty
- Single word: Treated as a single keyword
- Technical terms: Preserves compound technical terms (e.g., "React.js", "machine learning")
- Case differences: "JavaScript" matches "javascript"
- Common words: Ignored in scoring to focus on meaningful keywords

## Example

Evaluate keyword coverage between input queries and agent responses:

```typescript title="src/example-keyword-coverage.ts"
import { runEvals } from "@mastra/core/evals";
import { createKeywordCoverageScorer } from "@mastra/evals/scorers/prebuilt";
import { myAgent } from "./agent";

const scorer = createKeywordCoverageScorer();

const result = await runEvals({
  data: [
    {
      input: "JavaScript frameworks like React and Vue",
    },
    {
      input: "TypeScript offers interfaces, generics, and type inference",
    },
    {
      input:
        "Machine learning models require data preprocessing, feature engineering, and hyperparameter tuning",
    },
  ],
  scorers: [scorer],
  target: myAgent,
  onItemComplete: ({ scorerResults }) => {
    console.log({
      score: scorerResults[scorer.id].score,
    });
  },
});

console.log(result.scores);
```

For more details on `runEvals`, see the [runEvals reference](https://mastra.ai/reference/v1/evals/run-evals).

To add this scorer to an agent, see the [Scorers overview](https://mastra.ai/docs/v1/evals/overview#adding-scorers-to-agents) guide.

## Related

- [Completeness Scorer](./completeness)
- [Content Similarity Scorer](./content-similarity)
- [Answer Relevancy Scorer](./answer-relevancy)
- [Textual Difference Scorer](./textual-difference)

---

## Reference: MastraScorer

> Documentation for the MastraScorer base class in Mastra, which provides the foundation for all custom and built-in scorers.

The `MastraScorer` class is the base class for all scorers in Mastra. It provides a standard `.run()` method for evaluating input/output pairs and supports multi-step scoring workflows with preprocess → analyze → generateScore → generateReason execution flow.

**Note:** Most users should use [`createScorer`](./create-scorer) to create scorer instances. Direct instantiation of `MastraScorer` is not recommended.

## How to Get a MastraScorer Instance

Use the `createScorer` factory function, which returns a `MastraScorer` instance:

```typescript
import { createScorer } from "@mastra/core/evals";

const scorer = createScorer({
  name: "My Custom Scorer",
  description: "Evaluates responses based on custom criteria",
}).generateScore(({ run, results }) => {
  // scoring logic
  return 0.85;
});

// scorer is now a MastraScorer instance
```

## .run() Method

The `.run()` method is the primary way to execute your scorer and evaluate input/output pairs. It processes the data through your defined steps (preprocess → analyze → generateScore → generateReason) and returns a comprehensive result object with the score, reasoning, and intermediate results.

```typescript
const result = await scorer.run({
  input: "What is machine learning?",
  output: "Machine learning is a subset of artificial intelligence...",
  runId: "optional-run-id",
  requestContext: {
    /* optional context */
  },
});
```

## .run() Input

## .run() Returns

## Step Execution Flow

When you call `.run()`, the MastraScorer executes the defined steps in this order:

1. **preprocess** (optional) - Extracts or transforms data
2. **analyze** (optional) - Processes the input/output and preprocessed data
3. **generateScore** (required) - Computes the numerical score
4. **generateReason** (optional) - Provides explanation for the score

Each step receives the results from previous steps, allowing you to build complex evaluation pipelines.

## Usage Example

```typescript
const scorer = createScorer({
  name: "Quality Scorer",
  description: "Evaluates response quality",
})
  .preprocess(({ run }) => {
    // Extract key information
    return { wordCount: run.output.split(" ").length };
  })
  .analyze(({ run, results }) => {
    // Analyze the response
    const hasSubstance = results.preprocessStepResult.wordCount > 10;
    return { hasSubstance };
  })
  .generateScore(({ results }) => {
    // Calculate score
    return results.analyzeStepResult.hasSubstance ? 1.0 : 0.0;
  })
  .generateReason(({ score, results }) => {
    // Explain the score
    const wordCount = results.preprocessStepResult.wordCount;
    return `Score: ${score}. Response has ${wordCount} words.`;
  });

// Use the scorer
const result = await scorer.run({
  input: "What is machine learning?",
  output: "Machine learning is a subset of artificial intelligence...",
});

console.log(result.score); // 1.0
console.log(result.reason); // "Score: 1.0. Response has 12 words."
```

## Integration

MastraScorer instances can be used for agents and workflow steps

See the [createScorer reference](./create-scorer) for detailed information on defining custom scoring logic.

---

## Reference: runEvals

> Documentation for the runEvals function in Mastra, which enables batch evaluation of agents and workflows using multiple scorers.

The `runEvals` function enables batch evaluation of agents and workflows by running multiple test cases against scorers concurrently. This is essential for systematic testing, performance analysis, and validation of AI systems.

## Usage Example

```typescript
import { runEvals } from "@mastra/core/evals";
import { myAgent } from "./agents/my-agent";
import { myScorer1, myScorer2 } from "./scorers";

const result = await runEvals({
  target: myAgent,
  data: [
    { input: "What is machine learning?" },
    { input: "Explain neural networks" },
    { input: "How does AI work?" },
  ],
  scorers: [myScorer1, myScorer2],
  concurrency: 2,
  onItemComplete: ({ item, targetResult, scorerResults }) => {
    console.log(`Completed: ${item.input}`);
    console.log(`Scores:`, scorerResults);
  },
});

console.log(`Average scores:`, result.scores);
console.log(`Processed ${result.summary.totalItems} items`);
```

## Parameters

## Data Item Structure

## Workflow Scorer Configuration

For workflows, you can specify scorers at different levels using `WorkflowScorerConfig`:

## Returns

## Examples

### Agent Evaluation

```typescript
import { createScorer, runEvals } from "@mastra/core/evals";

const myScorer = createScorer({
  id: "my-scorer",
  description: "Check if Agent's response contains ground truth",
  type: "agent",
}).generateScore(({ run }) => {
  const response = run.output[0]?.content || "";
  const expectedResponse = run.groundTruth;
  return response.includes(expectedResponse) ? 1 : 0;
});

const result = await runEvals({
  target: chatAgent,
  data: [
    {
      input: "What is AI?",
      groundTruth:
        "AI is a field of computer science that creates intelligent machines.",
    },
    {
      input: "How does machine learning work?",
      groundTruth:
        "Machine learning uses algorithms to learn patterns from data.",
    },
  ],
  scorers: [relevancyScorer],
  concurrency: 3,
});
```

### Workflow Evaluation

```typescript
const workflowResult = await runEvals({
  target: myWorkflow,
  data: [
    { input: { query: "Process this data", priority: "high" } },
    { input: { query: "Another task", priority: "low" } },
  ],
  scorers: {
    workflow: [outputQualityScorer],
    steps: {
      "validation-step": [validationScorer],
      "processing-step": [processingScorer],
    },
  },
  onItemComplete: ({ item, targetResult, scorerResults }) => {
    console.log(`Workflow completed for: ${item.inputData.query}`);
    if (scorerResults.workflow) {
      console.log("Workflow scores:", scorerResults.workflow);
    }
    if (scorerResults.steps) {
      console.log("Step scores:", scorerResults.steps);
    }
  },
});
```

## Related

- [createScorer()](https://mastra.ai/reference/v1/evals/create-scorer) - Create custom scorers for experiments
- [MastraScorer](https://mastra.ai/reference/v1/evals/mastra-scorer) - Learn about scorer structure and methods
- [Custom Scorers](https://mastra.ai/docs/v1/evals/custom-scorers) - Guide to building evaluation logic
- [Scorers Overview](https://mastra.ai/docs/v1/evals/overview) - Understanding scorer concepts

---

## Reference: Scorer Utils

> Utility functions for extracting data from scorer run inputs and outputs, including text content, reasoning, system messages, and tool calls.

Mastra provides utility functions to help extract and process data from scorer run inputs and outputs. These utilities are particularly useful in the `preprocess` step of custom scorers.

## Import

```typescript
import {
  getAssistantMessageFromRunOutput,
  getReasoningFromRunOutput,
  getUserMessageFromRunInput,
  getSystemMessagesFromRunInput,
  getCombinedSystemPrompt,
  extractToolCalls,
  extractInputMessages,
  extractAgentResponseMessages,
} from "@mastra/evals/scorers/utils";
```

## Message Extraction

### getAssistantMessageFromRunOutput

Extracts the text content from the first assistant message in the run output.

```typescript
const scorer = createScorer({
  id: "my-scorer",
  description: "My scorer",
  type: "agent",
})
  .preprocess(({ run }) => {
    const response = getAssistantMessageFromRunOutput(run.output);
    return { response };
  })
  .generateScore(({ results }) => {
    return results.preprocessStepResult?.response ? 1 : 0;
  });
```

**Returns:** `string | undefined` - The assistant message text, or undefined if no assistant message is found.

### getUserMessageFromRunInput

Extracts the text content from the first user message in the run input.

```typescript
.preprocess(({ run }) => {
  const userMessage = getUserMessageFromRunInput(run.input);
  return { userMessage };
})
```

**Returns:** `string | undefined` - The user message text, or undefined if no user message is found.

### extractInputMessages

Extracts text content from all input messages as an array.

```typescript
.preprocess(({ run }) => {
  const allUserMessages = extractInputMessages(run.input);
  return { conversationHistory: allUserMessages.join("\n") };
})
```

**Returns:** `string[]` - Array of text strings from each input message.

### extractAgentResponseMessages

Extracts text content from all assistant response messages as an array.

```typescript
.preprocess(({ run }) => {
  const allResponses = extractAgentResponseMessages(run.output);
  return { allResponses };
})
```

**Returns:** `string[]` - Array of text strings from each assistant message.

## Reasoning Extraction

### getReasoningFromRunOutput

Extracts reasoning text from the run output. This is particularly useful when evaluating responses from reasoning models like `deepseek-reasoner` that produce chain-of-thought reasoning.

Reasoning can be stored in two places:
1. `content.reasoning` - a string field on the message content
2. `content.parts` - as parts with `type: 'reasoning'` containing `details`

```typescript
import { 
  getReasoningFromRunOutput, 
  getAssistantMessageFromRunOutput 
} from "@mastra/evals/scorers/utils";

const reasoningQualityScorer = createScorer({
  id: "reasoning-quality",
  name: "Reasoning Quality",
  description: "Evaluates the quality of model reasoning",
  type: "agent",
})
  .preprocess(({ run }) => {
    const reasoning = getReasoningFromRunOutput(run.output);
    const response = getAssistantMessageFromRunOutput(run.output);
    return { reasoning, response };
  })
  .analyze(({ results }) => {
    const { reasoning } = results.preprocessStepResult || {};
    return {
      hasReasoning: !!reasoning,
      reasoningLength: reasoning?.length || 0,
      hasStepByStep: reasoning?.includes("step") || false,
    };
  })
  .generateScore(({ results }) => {
    const { hasReasoning, reasoningLength } = results.analyzeStepResult || {};
    if (!hasReasoning) return 0;
    // Score based on reasoning length (normalized to 0-1)
    return Math.min(reasoningLength / 500, 1);
  })
  .generateReason(({ results, score }) => {
    const { hasReasoning, reasoningLength } = results.analyzeStepResult || {};
    if (!hasReasoning) {
      return "No reasoning was provided by the model.";
    }
    return `Model provided ${reasoningLength} characters of reasoning. Score: ${score}`;
  });
```

**Returns:** `string | undefined` - The reasoning text, or undefined if no reasoning is present.

## System Message Extraction

### getSystemMessagesFromRunInput

Extracts all system messages from the run input, including both standard system messages and tagged system messages (specialized prompts like memory instructions).

```typescript
.preprocess(({ run }) => {
  const systemMessages = getSystemMessagesFromRunInput(run.input);
  return { 
    systemPromptCount: systemMessages.length,
    systemPrompts: systemMessages 
  };
})
```

**Returns:** `string[]` - Array of system message strings.

### getCombinedSystemPrompt

Combines all system messages into a single prompt string, joined with double newlines.

```typescript
.preprocess(({ run }) => {
  const fullSystemPrompt = getCombinedSystemPrompt(run.input);
  return { fullSystemPrompt };
})
```

**Returns:** `string` - Combined system prompt string.

## Tool Call Extraction

### extractToolCalls

Extracts information about all tool calls from the run output, including tool names, call IDs, and their positions in the message array.

```typescript
const toolUsageScorer = createScorer({
  id: "tool-usage",
  description: "Evaluates tool usage patterns",
  type: "agent",
})
  .preprocess(({ run }) => {
    const { tools, toolCallInfos } = extractToolCalls(run.output);
    return {
      toolsUsed: tools,
      toolCount: tools.length,
      toolDetails: toolCallInfos,
    };
  })
  .generateScore(({ results }) => {
    const { toolCount } = results.preprocessStepResult || {};
    // Score based on appropriate tool usage
    return toolCount > 0 ? 1 : 0;
  });
```

**Returns:**

```typescript
{
  tools: string[];           // Array of tool names
  toolCallInfos: ToolCallInfo[];  // Detailed tool call information
}
```

Where `ToolCallInfo` is:

```typescript
type ToolCallInfo = {
  toolName: string;      // Name of the tool
  toolCallId: string;    // Unique call identifier
  messageIndex: number;  // Index in the output array
  invocationIndex: number; // Index within message's tool invocations
};
```

## Test Utilities

These utilities help create test data for scorer development.

### createTestMessage

Creates a `MastraDBMessage` object for testing purposes.

```typescript
import { createTestMessage } from "@mastra/evals/scorers/utils";

const userMessage = createTestMessage({
  content: "What is the weather?",
  role: "user",
});

const assistantMessage = createTestMessage({
  content: "The weather is sunny.",
  role: "assistant",
  toolInvocations: [
    {
      toolCallId: "call-1",
      toolName: "weatherTool",
      args: { location: "London" },
      result: { temp: 20 },
      state: "result",
    },
  ],
});
```

### createAgentTestRun

Creates a complete test run object for testing scorers.

```typescript
import { createAgentTestRun, createTestMessage } from "@mastra/evals/scorers/utils";

const testRun = createAgentTestRun({
  inputMessages: [
    createTestMessage({ content: "Hello", role: "user" }),
  ],
  output: [
    createTestMessage({ content: "Hi there!", role: "assistant" }),
  ],
});

// Run your scorer with the test data
const result = await myScorer.run({
  input: testRun.input,
  output: testRun.output,
});
```

## Complete Example

Here's a complete example showing how to use multiple utilities together:

```typescript
import { createScorer } from "@mastra/core/evals";
import {
  getAssistantMessageFromRunOutput,
  getReasoningFromRunOutput,
  getUserMessageFromRunInput,
  getCombinedSystemPrompt,
  extractToolCalls,
} from "@mastra/evals/scorers/utils";

const comprehensiveScorer = createScorer({
  id: "comprehensive-analysis",
  name: "Comprehensive Analysis",
  description: "Analyzes all aspects of an agent response",
  type: "agent",
})
  .preprocess(({ run }) => {
    // Extract all relevant data
    const userMessage = getUserMessageFromRunInput(run.input);
    const response = getAssistantMessageFromRunOutput(run.output);
    const reasoning = getReasoningFromRunOutput(run.output);
    const systemPrompt = getCombinedSystemPrompt(run.input);
    const { tools, toolCallInfos } = extractToolCalls(run.output);

    return {
      userMessage,
      response,
      reasoning,
      systemPrompt,
      toolsUsed: tools,
      toolCount: tools.length,
    };
  })
  .generateScore(({ results }) => {
    const { response, reasoning, toolCount } = results.preprocessStepResult || {};
    
    let score = 0;
    if (response && response.length > 0) score += 0.4;
    if (reasoning) score += 0.3;
    if (toolCount > 0) score += 0.3;
    
    return score;
  })
  .generateReason(({ results, score }) => {
    const { response, reasoning, toolCount } = results.preprocessStepResult || {};
    
    const parts = [];
    if (response) parts.push("provided a response");
    if (reasoning) parts.push("included reasoning");
    if (toolCount > 0) parts.push(`used ${toolCount} tool(s)`);
    
    return `Score: ${score}. The agent ${parts.join(", ")}.`;
  });
```

---

## Reference: Textual Difference Scorer

> Documentation for the Textual Difference Scorer in Mastra, which measures textual differences between strings using sequence matching.

The `createTextualDifferenceScorer()` function uses sequence matching to measure the textual differences between two strings. It provides detailed information about changes, including the number of operations needed to transform one text into another.

## Parameters

The `createTextualDifferenceScorer()` function does not take any options.

This function returns an instance of the MastraScorer class. See the [MastraScorer reference](./mastra-scorer) for details on the `.run()` method and its input/output.

## .run() Returns

`.run()` returns a result in the following shape:

```typescript
{
  runId: string,
  analyzeStepResult: {
    confidence: number,
    ratio: number,
    changes: number,
    lengthDiff: number
  },
  score: number
}
```

## Scoring Details

The scorer calculates several measures:

- **Similarity Ratio**: Based on sequence matching between texts (0-1)
- **Changes**: Count of non-matching operations needed
- **Length Difference**: Normalized difference in text lengths
- **Confidence**: Inversely proportional to length difference

### Scoring Process

1. Analyzes textual differences:
   - Performs sequence matching between input and output
   - Counts the number of change operations required
   - Measures length differences
2. Calculates metrics:
   - Computes similarity ratio
   - Determines confidence score
   - Combines into weighted score

Final score: `(similarity_ratio * confidence) * scale`

### Score interpretation

A textual difference score between 0 and 1:

- **1.0**: Identical texts – no differences detected.
- **0.7–0.9**: Minor differences – few changes needed.
- **0.4–0.6**: Moderate differences – noticeable changes required.
- **0.1–0.3**: Major differences – extensive changes needed.
- **0.0**: Completely different texts.

## Example

Measure textual differences between expected and actual agent outputs:

```typescript title="src/example-textual-difference.ts"
import { runEvals } from "@mastra/core/evals";
import { createTextualDifferenceScorer } from "@mastra/evals/scorers/prebuilt";
import { myAgent } from "./agent";

const scorer = createTextualDifferenceScorer();

const result = await runEvals({
  data: [
    {
      input: "Summarize the concept of recursion",
      groundTruth:
        "Recursion is when a function calls itself to solve a problem by breaking it into smaller subproblems.",
    },
    {
      input: "What is the capital of France?",
      groundTruth: "The capital of France is Paris.",
    },
  ],
  scorers: [scorer],
  target: myAgent,
  onItemComplete: ({ scorerResults }) => {
    console.log({
      score: scorerResults[scorer.id].score,
      groundTruth: scorerResults[scorer.id].groundTruth,
    });
  },
});

console.log(result.scores);
```

For more details on `runEvals`, see the [runEvals reference](https://mastra.ai/reference/v1/evals/run-evals).

To add this scorer to an agent, see the [Scorers overview](https://mastra.ai/docs/v1/evals/overview#adding-scorers-to-agents) guide.

## Related

- [Content Similarity Scorer](./content-similarity)
- [Completeness Scorer](./completeness)
- [Keyword Coverage Scorer](./keyword-coverage)

---

## Reference: Tone Consistency Scorer

> Documentation for the Tone Consistency Scorer in Mastra, which evaluates emotional tone and sentiment consistency in text.

The `createToneScorer()` function evaluates the text's emotional tone and sentiment consistency. It can operate in two modes: comparing tone between input/output pairs or analyzing tone stability within a single text.

## Parameters

The `createToneScorer()` function does not take any options.

This function returns an instance of the MastraScorer class. See the [MastraScorer reference](./mastra-scorer) for details on the `.run()` method and its input/output.

## .run() Returns

`.run()` returns a result in the following shape:

```typescript
{
  runId: string,
  analyzeStepResult: {
    responseSentiment?: number,
    referenceSentiment?: number,
    difference?: number,
    avgSentiment?: number,
    sentimentVariance?: number,
  },
  score: number
}
```

## Scoring Details

The scorer evaluates sentiment consistency through tone pattern analysis and mode-specific scoring.

### Scoring Process

1. Analyzes tone patterns:
   - Extracts sentiment features
   - Computes sentiment scores
   - Measures tone variations
2. Calculates mode-specific score:
   **Tone Consistency** (input and output):
   - Compares sentiment between texts
   - Calculates sentiment difference
   - Score = 1 - (sentiment_difference / max_difference)
     **Tone Stability** (single input):
   - Analyzes sentiment across sentences
   - Calculates sentiment variance
   - Score = 1 - (sentiment_variance / max_variance)

Final score: `mode_specific_score * scale`

### Score interpretation

(0 to scale, default 0-1)

- 1.0: Perfect tone consistency/stability
- 0.7-0.9: Strong consistency with minor variations
- 0.4-0.6: Moderate consistency with noticeable shifts
- 0.1-0.3: Poor consistency with major tone changes
- 0.0: No consistency - completely different tones

### analyzeStepResult

Object with tone metrics:

- **responseSentiment**: Sentiment score for the response (comparison mode).
- **referenceSentiment**: Sentiment score for the input/reference (comparison mode).
- **difference**: Absolute difference between sentiment scores (comparison mode).
- **avgSentiment**: Average sentiment across sentences (stability mode).
- **sentimentVariance**: Variance of sentiment across sentences (stability mode).

## Example

Evaluate tone consistency between related agent responses:

```typescript title="src/example-tone-consistency.ts"
import { runEvals } from "@mastra/core/evals";
import { createToneScorer } from "@mastra/evals/scorers/prebuilt";
import { myAgent } from "./agent";

const scorer = createToneScorer();

const result = await runEvals({
  data: [
    {
      input: "How was your experience with our service?",
      groundTruth: "The service was excellent and exceeded expectations!",
    },
    {
      input: "Tell me about the customer support",
      groundTruth: "The support team was friendly and very helpful.",
    },
  ],
  scorers: [scorer],
  target: myAgent,
  onItemComplete: ({ scorerResults }) => {
    console.log({
      score: scorerResults[scorer.id].score,
    });
  },
});

console.log(result.scores);
```

For more details on `runEvals`, see the [runEvals reference](https://mastra.ai/reference/v1/evals/run-evals).

To add this scorer to an agent, see the [Scorers overview](https://mastra.ai/docs/v1/evals/overview#adding-scorers-to-agents) guide.

## Related

- [Content Similarity Scorer](./content-similarity)
- [Toxicity Scorer](./toxicity)

---

## Reference: Toxicity Scorer

> Documentation for the Toxicity Scorer in Mastra, which evaluates LLM outputs for racist, biased, or toxic elements.

The `createToxicityScorer()` function evaluates whether an LLM's output contains racist, biased, or toxic elements. It uses a judge-based system to analyze responses for various forms of toxicity including personal attacks, mockery, hate speech, dismissive statements, and threats.

## Parameters

The `createToxicityScorer()` function accepts a single options object with the following properties:

This function returns an instance of the MastraScorer class. The `.run()` method accepts the same input as other scorers (see the [MastraScorer reference](./mastra-scorer)), but the return value includes LLM-specific fields as documented below.

## .run() Returns

`.run()` returns a result in the following shape:

```typescript
{
  runId: string,
  analyzeStepResult: {
    verdicts: Array<{ verdict: 'yes' | 'no', reason: string }>
  },
  analyzePrompt: string,
  score: number,
  reason: string,
  reasonPrompt: string
}
```

## Scoring Details

The scorer evaluates toxicity through multiple aspects:

- Personal attacks
- Mockery or sarcasm
- Hate speech
- Dismissive statements
- Threats or intimidation

### Scoring Process

1. Analyzes toxic elements:
   - Identifies personal attacks and mockery
   - Detects hate speech and threats
   - Evaluates dismissive statements
   - Assesses severity levels
2. Calculates toxicity score:
   - Weighs detected elements
   - Combines severity ratings
   - Normalizes to scale

Final score: `(toxicity_weighted_sum / max_toxicity) * scale`

### Score interpretation

A toxicity score between 0 and 1:

- **0.8–1.0**: Severe toxicity.
- **0.4–0.7**: Moderate toxicity.
- **0.1–0.3**: Mild toxicity.
- **0.0**: No toxic elements detected.

## Example

Evaluate agent responses for toxic, biased, or harmful content:

```typescript title="src/example-toxicity.ts"
import { runEvals } from "@mastra/core/evals";
import { createToxicityScorer } from "@mastra/evals/scorers/prebuilt";
import { myAgent } from "./agent";

const scorer = createToxicityScorer({ model: "openai/gpt-4o" });

const result = await runEvals({
  data: [
    {
      input: "What do you think about the new team member?",
    },
    {
      input: "How was the meeting discussion?",
    },
    {
      input: "Can you provide feedback on the project proposal?",
    },
  ],
  scorers: [scorer],
  target: myAgent,
  onItemComplete: ({ scorerResults }) => {
    console.log({
      score: scorerResults[scorer.id].score,
      reason: scorerResults[scorer.id].reason,
    });
  },
});

console.log(result.scores);
```

For more details on `runEvals`, see the [runEvals reference](https://mastra.ai/reference/v1/evals/run-evals).

To add this scorer to an agent, see the [Scorers overview](https://mastra.ai/docs/v1/evals/overview#adding-scorers-to-agents) guide.

## Related

- [Tone Consistency Scorer](./tone-consistency)
- [Bias Scorer](./bias)